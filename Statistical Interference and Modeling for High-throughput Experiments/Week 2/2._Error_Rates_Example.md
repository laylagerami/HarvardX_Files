Error Rates Example
================

In this video we use MC simulation to illustrate error rates. We are going to use the female mouse weights control population and simulate an example where we try 10,000 different diets to see if they have an effect on weight. We will generate them as if the null hypothesis is true for all diets by sampling from the same population calling 12 of them cases and 12 of them controls, though we know there is no difference. So in this case m = 10,000 (number of tests) and m0 = 10,000 because the null hypothesis is true for all of them.

``` r
set.seed(1)
population = unlist(read.csv("femaleControlsPopulation.csv")) 
alpha <- 0.05
N <- 12
m <- 10000
pvals <- replicate(m,{
  control = sample(population,N)
  treatment = sample(population,N)
  t.test(treatment,control)$p.value
})
sum( pvals < alpha )#This is R
```

    ## [1] 462

In this case, we computed R which is the number of times that the p-values are less than 0.05. So this is our procedure we are defining. We will now look at the error rates for this procedure. So we have 462 cases we can call significant. In this case we know the null is true for all the tests, this is V. So V and R are the same thing.

``` r
alpha <- 0.05
N < - 12 #sample size 12
```

    ## [1] FALSE

``` r
m <- 10000 #number of repetitions total
p0 <- 0.90 #10% of diets work 90% don't
m0 <- m*p0 #9000 NH true
m1 <- m-m0 #1000 AH true
nullHypothesis <- c(rep(TRUE,m0),rep(FALSE,m1)) #vector with 9000 x TRUE and 1000 x FALSE
delta <- 3 #When NH is false the difference btwn case and control is 3g so we add this value
```

Let's construct a slightly more interesting example where 10% of the diets work. For 1000 diets, there is a difference between the two diets that is 3 grams higher on average. The null hypothesis is a vector that has 9,000 true (NH is true) and 1,000 false (NH is false). Now when we create the simulation when NH is false, we add delta (difference between cases and control when the diet has an effect).

``` r
set.seed(1)
calls <- sapply(1:m,function(i){ #sapply from 1-10,000 on list/vector of data, returns vector
  control <- sample(population,N) #sampling
  treatment <- sample(population,N) #sampling
  if(!nullHypothesis[i]) treatment <- treatment + delta #if nullHypothesis entry [i] is false, AH is true so add delta to the treatment samples
  ifelse(t.test(treatment,control)$p.value < alpha, #if pvalue < alpha
         "Called significant", #then it is significant
         "Not called significant") #else it is not significant 
})
#We now have a vector (calls) of called significant (reject NH) or not called significant (accept NH) for each test
```

Now we have run 10,000 tests, for each one we called it significant or not. But we have another piece of information. We usually don't know when the null hypothesis is true, but here we do. So we are going to turn that into a factor, the null hypothesis lists.

``` r
null_hypothesis <- factor(nullHypothesis,levels=c("TRUE","FALSE")) 
#Factors are variables in R which take on categorical values. They are stored as a vector of integer values with a corresponding set of character values to use when the factor is displayed. For example this factor has the values from the nullHypothesis vector in the order TRUE then FALSE.
table(null_hypothesis,calls)
```

    ##                calls
    ## null_hypothesis Called significant Not called significant
    ##           TRUE                 421                   8579
    ##           FALSE                520                    480

``` r
#Creates a table from the null_hypothesis factor (TRUE and FALSE) and the calls vector (Called significant and Not called significant) showing the total number of tests in each cell.
```

Now we can see V and S and m etc. We have cases in which the null hypothesis is true, but we called it significant, thats 421 false positives (V). Then we have cases where the alternative is true and we called it significant - true positives (520). We have type II errors, where the alternative was true but we called it not significant (480). This is just one instance, we can repeat it over and over and V will change because V is a random variable. This is why we can talk about the familywise error rate, what is the probability that V is bigger than 1? We are going to run the same thing 10 more times and see what happens to V and S.

``` r
B <- 10
VandS <- replicate(B,{ #replicate "calls" from above 10 times
  calls <- sapply(1:m, function(i){
    control <- sample(population,N)
    treatment <- sample(population,N)
    if(!nullHypothesis[i]) treatment <- treatment + delta
    t.test(treatment,control)$p.val < alpha
  })
  cat("V = ",sum(nullHypothesis & calls), "S=",sum(!nullHypothesis & calls),"\n") 
  #V is the number of false positives - when NH is true and p value < alpha
  #S is the number of true positives - when  NH is false and p value < alpha
})
```

    ## V =  410 S= 564 
    ## V =  400 S= 552 
    ## V =  366 S= 546 
    ## V =  382 S= 553 
    ## V =  372 S= 505 
    ## V =  382 S= 530 
    ## V =  381 S= 539 
    ## V =  396 S= 554 
    ## V =  380 S= 550 
    ## V =  405 S= 569

They are different every time which is why we can talk about rates. For example, the probability of V being bigger or equal to 1 (familywise error rate) is clear that it is going to be very high. Therefore, this procedure has a very high familywise error rate.
